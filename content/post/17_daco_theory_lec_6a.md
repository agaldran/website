+++
date = 2017-09-11
draft = false
tags = ["daco_theory_17"]
title = "DACO - Theoretical Lecture 6a - Linear Models"
math = false
summary = """
Where we introduce the one of the most simple machine learning models, and show how to obtain a numerical solution. 
"""
+++

Today's lecture is split in two parts. 

In the first part, you will be introduced to a simple, but fundamental kind of machine learning models, Linear models.
We will understand how are they formulated, and how can they be solved numerically. 
We will also see how can they be used for both regression and classification.
 
The second part comprises an introduction to non-linear models, specifically to neural networks.
We will continue studying neural networks in the next lecture, but this week you will already understand what are they and how can they be used for classification tasks.
The material related to this second part is located [here](google.es)

This is an overview of what we will be learning in this first hour:

1. **Motivation and Goals. Linear Models for Supervised Classification and Regression**

2. **Linear Regression Models**
    1. Linear Regression for one variable
    2. Gradient descent for Linear Regression
    3. Linear Regression - A real-world example
    4. Linear Regression for more than one variable
    
3. **Logistic Regression Models (for classification!)**
    1. First part
    2. Second part

4. **Sources and References**


{{% alert note %}}
The same as in the practical lectures, I have also built a Jupyter notebook that contains all the information you will find below. 
In addition, it will allow you to experiment with short pieces of code. You can access it [here](google.es).
{{% /alert %}}